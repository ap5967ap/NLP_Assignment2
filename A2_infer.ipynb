{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization,ReLU\n",
    "from keras.optimizers import Adam,SGD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import emoji\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "import string\n",
    "from keras.utils import to_categorical\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ap/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ap/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "path = \"./sentimentdataset.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "def preprocess(df):\n",
    "    df['Sentiment'] = df['Sentiment'].str.strip().str.replace(r'\\s+', '', regex=True)\n",
    "    class_counts = df['Sentiment'].value_counts()\n",
    "    rare_classes = class_counts[class_counts < 2].index\n",
    "    balanced_df = df.copy()\n",
    "    for rare_class in rare_classes:\n",
    "        rare_samples = df[df['Sentiment'] == rare_class]\n",
    "        duplicated_samples = resample(rare_samples, replace=True, n_samples=5, random_state=42)\n",
    "        balanced_df = pd.concat([balanced_df, duplicated_samples], ignore_index=True)\n",
    "    df = balanced_df\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    def preprocess_text(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = emoji.replace_emoji(text, replace='')\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        return \" \".join(tokens)\n",
    "    df1=df.copy()\n",
    "    df1['clean_text'] = df1['Text'].apply(preprocess_text)\n",
    "    df1['clean_text'].head()\n",
    "    df1['label_num'] = df1['Sentiment'].astype('category').cat.codes\n",
    "    _, X_test_df1, __, y_test_df1 = train_test_split(\n",
    "            df1['clean_text'], df1['label_num'], test_size=0.2, random_state=42, stratify=df1['label_num']\n",
    "    )\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    _ = vectorizer.fit_transform(_)\n",
    "    X_test_tfidf1 = vectorizer.transform(X_test_df1)\n",
    "    return X_test_tfidf1,y_test_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model1=load_model('./part2_model1.keras')\n",
    "model2=load_model('./part2_model2.keras')\n",
    "model3=load_model('./part2_model3.keras')\n",
    "model4=load_model('./part2_model4.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_test11 = to_categorical(y_test, num_classes=191)\n",
    "    y_test_labels = np.argmax(y_test11, axis=1)\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "    return accuracy_score(y_test_labels, y_pred_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf1,y_test_df1=preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "0.6858407079646017 0.6902654867256637 0.7035398230088495 0.7345132743362832\n"
     ]
    }
   ],
   "source": [
    "print(predict(model1,X_test_tfidf1,y_test_df1),predict(model2,X_test_tfidf1,y_test_df1),predict(model3,X_test_tfidf1,y_test_df1),predict(model4,X_test_tfidf1,y_test_df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
